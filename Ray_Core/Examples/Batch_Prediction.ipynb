{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdc2d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://docs.ray.io/en/latest/ray-core/examples/batch_prediction.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f76eec",
   "metadata": {},
   "source": [
    "# Task-based batch prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db677f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_model():\n",
    "    # A dummy model.\n",
    "    def model(batch: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Dummy payload so copying the model will actually copy some data\n",
    "        # across nodes.\n",
    "        model.payload = np.zeros(100_000_000)\n",
    "        return pd.DataFrame({\"score\": batch[\"passenger_count\"] % 2 == 0})\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34bf19c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import ray\n",
    "\n",
    "@ray.remote\n",
    "def make_prediction(model, shard_path):\n",
    "    df = pq.read_table(shard_path).to_pandas()\n",
    "    result = model(df)\n",
    "\n",
    "    # Write out the prediction result.\n",
    "    # NOTE: unless the driver will have to further process the\n",
    "    # result (other than simply writing out to storage system),\n",
    "    # writing out at remote task is recommended, as it can avoid\n",
    "    # congesting or overloading the driver.\n",
    "    # ...\n",
    "\n",
    "    # Here we just return the size about the result in this example.\n",
    "    return len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e00ed4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-12 18:43:55,194\tINFO worker.py:1230 -- Using address localhost:9031 set in the environment variable RAY_ADDRESS\n",
      "2022-12-12 18:43:55,665\tINFO worker.py:1352 -- Connecting to existing Ray cluster at address: 10.0.63.8:9031...\n",
      "2022-12-12 18:43:55,683\tINFO worker.py:1529 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://console.anyscale.com/api/v2/sessions/ses_buwxbm99nq8dryqg6p8sbytw/services?redirect_to=dashboard \u001b[39m\u001b[22m\n",
      "2022-12-12 18:43:55,692\tINFO packaging.py:373 -- Pushing file package 'gcs://_ray_pkg_5bb1f25bbe9ee2fb06464440d8021e4c.zip' (0.11MiB) to Ray cluster...\n",
      "2022-12-12 18:43:55,695\tINFO packaging.py:386 -- Successfully pushed file package 'gcs://_ray_pkg_5bb1f25bbe9ee2fb06464440d8021e4c.zip'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction output size: 141062\n",
      "Prediction output size: 133932\n",
      "Prediction output size: 144014\n",
      "Prediction output size: 143087\n",
      "Prediction output size: 148108\n",
      "Prediction output size: 141981\n",
      "Prediction output size: 136394\n",
      "Prediction output size: 136999\n",
      "Prediction output size: 139985\n",
      "Prediction output size: 156198\n",
      "Prediction output size: 142893\n",
      "Prediction output size: 145976\n",
      "(scheduler +12s) Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n",
      "(scheduler +12s) Adding 1 node(s) of type worker-node-type-0.\n",
      "(scheduler +2m22s) Resized to 24 CPUs.\n"
     ]
    }
   ],
   "source": [
    "# 12 files, one for each remote task.\n",
    "input_files = [\n",
    "        f\"s3://anonymous@air-example-data/ursa-labs-taxi-data/downsampled_2009_full_year_data.parquet\"\n",
    "        f\"/fe41422b01c04169af2a65a83b753e0f_{i:06d}.parquet\"\n",
    "        for i in range(12)\n",
    "]\n",
    "\n",
    "# ray.put() the model just once to local object store, and then pass the\n",
    "# reference to the remote tasks.\n",
    "model = load_model()\n",
    "model_ref = ray.put(model)\n",
    "\n",
    "result_refs = []\n",
    "\n",
    "# Launch all prediction tasks.\n",
    "for file in input_files:\n",
    "    # Launch a prediction task by passing model reference and shard file to it.\n",
    "    # NOTE: it would be highly inefficient if you are passing the model itself\n",
    "    # like make_prediction.remote(model, file), which in order to pass the model\n",
    "    # to remote node will ray.put(model) for each task, potentially overwhelming\n",
    "    # the local object store and causing out-of-disk error.\n",
    "    result_refs.append(make_prediction.remote(model_ref, file))\n",
    "\n",
    "results = ray.get(result_refs)\n",
    "\n",
    "# Let's check prediction output size.\n",
    "for r in results:\n",
    "    print(\"Prediction output size:\", r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ffc096",
   "metadata": {},
   "source": [
    "# Actor-based batch prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbf55fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import ray\n",
    "\n",
    "@ray.remote\n",
    "class BatchPredictor:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    def predict(self, shard_path):\n",
    "        df = pq.read_table(shard_path).to_pandas()\n",
    "        result =self.model(df)\n",
    "\n",
    "        # Write out the prediction result.\n",
    "        # NOTE: unless the driver will have to further process the\n",
    "        # result (other than simply writing out to storage system),\n",
    "        # writing out at remote task is recommended, as it can avoid\n",
    "        # congesting or overloading the driver.\n",
    "        # ...\n",
    "\n",
    "        # Here we just return the size about the result in this example.\n",
    "        return len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b936ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction output size: 141062\n",
      "Prediction output size: 133932\n",
      "Prediction output size: 144014\n",
      "Prediction output size: 143087\n",
      "Prediction output size: 148108\n",
      "Prediction output size: 141981\n",
      "Prediction output size: 136394\n",
      "Prediction output size: 136999\n",
      "Prediction output size: 139985\n",
      "Prediction output size: 156198\n",
      "Prediction output size: 142893\n",
      "Prediction output size: 145976\n",
      "(scheduler +7m25s) Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "(scheduler +7m36s) Resized to 8 CPUs.\n"
     ]
    }
   ],
   "source": [
    "from ray.util.actor_pool import ActorPool\n",
    "\n",
    "model = load_model()\n",
    "model_ref = ray.put(model)\n",
    "num_actors = 4\n",
    "actors = [BatchPredictor.remote(model_ref) for _ in range(num_actors)]\n",
    "pool = ActorPool(actors)\n",
    "input_files = [\n",
    "        f\"s3://anonymous@air-example-data/ursa-labs-taxi-data/downsampled_2009_full_year_data.parquet\"\n",
    "        f\"/fe41422b01c04169af2a65a83b753e0f_{i:06d}.parquet\"\n",
    "        for i in range(12)\n",
    "]\n",
    "for file in input_files:\n",
    "    pool.submit(lambda a, v: a.predict.remote(v), file)\n",
    "while pool.has_next():\n",
    "    print(\"Prediction output size:\", pool.get_next())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071cbae0",
   "metadata": {},
   "source": [
    "# Batch prediction with GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95cc466c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "@ray.remote(num_gpus=1)\n",
    "def make_torch_prediction(model: torch.nn.Module, shard_path):\n",
    "    # Move model to GPU.\n",
    "    model.to(torch.device(\"cuda\"))\n",
    "    inputs = pq.read_table(shard_path).to_pandas().to_numpy()\n",
    "\n",
    "    results = []\n",
    "    # for each tensor in inputs:\n",
    "    #   results.append(model(tensor))\n",
    "    #\n",
    "    # Write out the results right in task instead of returning back\n",
    "    # to the driver node (unless you have to), to avoid congest/overload\n",
    "    # driver node.\n",
    "    # ...\n",
    "\n",
    "    # Here we just return simple/light meta information.\n",
    "    return len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307c94fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
